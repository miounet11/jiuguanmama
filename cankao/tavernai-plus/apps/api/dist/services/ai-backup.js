"use strict";
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.aiService = void 0;
const axios_1 = __importDefault(require("axios"));
const server_1 = require("../server");
// å¤šæ¨¡å‹ LLM é…ç½® - ä»ç¯å¢ƒå˜é‡è·å–
const NEWAPI_BASE_URL = process.env.NEWAPI_BASE_URL || 'https://ttkk.inping.com/v1';
const NEWAPI_KEY = process.env.NEWAPI_KEY;
const DEFAULT_MODEL = process.env.DEFAULT_MODEL || 'grok-3';
const DEFAULT_MAX_TOKENS = parseInt(process.env.NEWAPI_MAX_TOKENS || '4000');
const DEFAULT_TEMPERATURE = parseFloat(process.env.NEWAPI_TEMPERATURE || '0.7');
const SUPPORTED_MODELS = {
    'grok-3': {
        id: 'grok-3',
        name: 'Grok-3',
        provider: 'newapi',
        baseUrl: NEWAPI_BASE_URL,
        apiKey: NEWAPI_KEY,
        maxTokens: 4000,
        temperature: 0.7,
        description: 'å¼ºå¤§çš„å¯¹è¯æ¨¡å‹ï¼Œæ“…é•¿åˆ›æ„å†™ä½œå’Œå¤æ‚æ¨ç†',
        features: ['å¯¹è¯', 'åˆ›æ„å†™ä½œ', 'ä»£ç ç”Ÿæˆ', 'é€»è¾‘æ¨ç†'],
        pricePer1k: 0.01
    },
    'gpt-4': {
        id: 'gpt-4',
        name: 'GPT-4',
        provider: 'newapi',
        baseUrl: NEWAPI_BASE_URL,
        apiKey: NEWAPI_KEY,
        maxTokens: 8000,
        temperature: 0.7,
        description: 'OpenAI æœ€å¼ºå¤§çš„æ¨¡å‹ï¼Œé€‚åˆå¤æ‚ä»»åŠ¡',
        features: ['å¯¹è¯', 'åˆ†æ', 'åˆ›ä½œ', 'ç¼–ç¨‹', 'æ¨ç†'],
        pricePer1k: 0.03
    },
    'gpt-3.5-turbo': {
        id: 'gpt-3.5-turbo',
        name: 'GPT-3.5 Turbo',
        provider: 'newapi',
        baseUrl: NEWAPI_BASE_URL,
        apiKey: NEWAPI_KEY,
        maxTokens: 4000,
        temperature: 0.7,
        description: 'å¿«é€Ÿå“åº”ï¼Œæˆæœ¬è¾ƒä½çš„å¯¹è¯æ¨¡å‹',
        features: ['å¯¹è¯', 'æ–‡æœ¬ç”Ÿæˆ', 'æ€»ç»“'],
        pricePer1k: 0.002
    },
    'claude-3': {
        id: 'claude-3',
        name: 'Claude 3',
        provider: 'newapi',
        baseUrl: NEWAPI_BASE_URL,
        apiKey: NEWAPI_KEY,
        maxTokens: 4000,
        temperature: 0.7,
        description: 'Anthropic çš„å®‰å…¨å¯é å¯¹è¯æ¨¡å‹',
        features: ['å¯¹è¯', 'åˆ†æ', 'å®‰å…¨å›å¤'],
        pricePer1k: 0.015
    },
    'gemini-pro': {
        id: 'gemini-pro',
        name: 'Gemini Pro',
        provider: 'newapi',
        baseUrl: NEWAPI_BASE_URL,
        apiKey: NEWAPI_KEY,
        maxTokens: 2048,
        temperature: 0.7,
        description: 'Google çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ”¯æŒæ–‡æœ¬å’Œå›¾åƒ',
        features: ['å¯¹è¯', 'å¤šæ¨¡æ€', 'åˆ†æ'],
        pricePer1k: 0.001
    }
};
// è¿æ¥ç¨³å®šæ€§é…ç½®
const RETRY_CONFIG = {
    maxRetries: 3,
    baseDelay: 1000, // 1ç§’
    maxDelay: 10000, // 10ç§’
    backoffMultiplier: 2
};
const CONNECTION_POOL_CONFIG = {
    maxConnections: 10,
    timeout: 30000, // 30ç§’è¶…æ—¶
    keepAlive: true,
    keepAliveMsecs: 60000 // 60ç§’keep-alive
};
// éªŒè¯å…³é”®é…ç½®
if (!NEWAPI_KEY) {
    console.error('âŒ é”™è¯¯: NEWAPI_KEY æœªé…ç½®ï¼ŒAIæœåŠ¡å°†ä¸å¯ç”¨');
}
else {
    console.log('âœ… Grok-3 LLM é…ç½®å·²åŠ è½½');
    console.log(`   Base URL: ${NEWAPI_BASE_URL}`);
    console.log(`   Model: ${DEFAULT_MODEL}`);
    console.log(`   Max Tokens: ${DEFAULT_MAX_TOKENS}`);
    console.log(`   Temperature: ${DEFAULT_TEMPERATURE}`);
    console.log(`   Retry Config: ${RETRY_CONFIG.maxRetries} retries, ${RETRY_CONFIG.baseDelay}ms base delay`);
}
// æŒ‡æ•°é€€é¿å»¶è¿Ÿè®¡ç®—
const calculateDelay = (attempt) => {
    const delay = RETRY_CONFIG.baseDelay * Math.pow(RETRY_CONFIG.backoffMultiplier, attempt);
    return Math.min(delay, RETRY_CONFIG.maxDelay);
};
// å»¶è¿Ÿå‡½æ•°
const sleep = (ms) => new Promise(resolve => setTimeout(resolve, ms));
// é‡è¯•åŒ…è£…å™¨
async function withRetry(operation, context = 'AI Service') {
    let lastError = null;
    for (let attempt = 0; attempt <= RETRY_CONFIG.maxRetries; attempt++) {
        try {
            const result = await operation();
            if (attempt > 0) {
                console.log(`âœ… ${context}: é‡è¯•æˆåŠŸ (ç¬¬ ${attempt + 1} æ¬¡å°è¯•)`);
            }
            return result;
        }
        catch (error) {
            lastError = error;
            const isRetriable = error.code === 'ECONNRESET' ||
                error.code === 'ECONNREFUSED' ||
                error.code === 'ETIMEDOUT' ||
                error.code === 'ENOTFOUND' ||
                (error.response?.status >= 500 && error.response?.status < 600) ||
                error.response?.status === 429; // Rate limit
            if (!isRetriable || attempt === RETRY_CONFIG.maxRetries) {
                console.error(`âŒ ${context}: æ“ä½œå¤±è´¥ (ç¬¬ ${attempt + 1} æ¬¡å°è¯•)`, {
                    error: error.message,
                    code: error.code,
                    status: error.response?.status
                });
                break;
            }
            const delay = calculateDelay(attempt);
            console.warn(`âš ï¸ ${context}: ç¬¬ ${attempt + 1} æ¬¡å°è¯•å¤±è´¥ï¼Œ${delay}ms åé‡è¯•`, {
                error: error.message,
                nextRetryIn: `${delay}ms`
            });
            await sleep(delay);
        }
    }
    throw lastError || new Error(`${context}: æ“ä½œå¤±è´¥`);
}
// åˆ›å»ºè¿æ¥æ± ä¼˜åŒ–çš„ axios å®ä¾‹
const aiClient = axios_1.default.create({
    baseURL: NEWAPI_BASE_URL,
    headers: {
        'Authorization': `Bearer ${NEWAPI_KEY}`,
        'Content-Type': 'application/json',
        'Connection': 'keep-alive'
    },
    timeout: CONNECTION_POOL_CONFIG.timeout,
    maxRedirects: 3,
    // è¿æ¥æ± é…ç½®
    httpAgent: new (require('http').Agent)({
        keepAlive: CONNECTION_POOL_CONFIG.keepAlive,
        keepAliveMsecs: CONNECTION_POOL_CONFIG.keepAliveMsecs,
        maxSockets: CONNECTION_POOL_CONFIG.maxConnections,
        timeout: CONNECTION_POOL_CONFIG.timeout
    }),
    httpsAgent: new (require('https').Agent)({
        keepAlive: CONNECTION_POOL_CONFIG.keepAlive,
        keepAliveMsecs: CONNECTION_POOL_CONFIG.keepAliveMsecs,
        maxSockets: CONNECTION_POOL_CONFIG.maxConnections,
        timeout: CONNECTION_POOL_CONFIG.timeout
    })
});
// è¯·æ±‚æ‹¦æˆªå™¨ - æ·»åŠ è¯·æ±‚IDå’Œæ—¥å¿—
aiClient.interceptors.request.use((config) => {
    const requestId = Math.random().toString(36).substr(2, 9);
    config.headers['X-Request-ID'] = requestId;
    console.log(`ğŸš€ AI API è¯·æ±‚ [${requestId}]:`, {
        method: config.method?.toUpperCase(),
        url: config.url,
        timeout: config.timeout
    });
    return config;
}, (error) => {
    console.error('âŒ AI API è¯·æ±‚é…ç½®é”™è¯¯:', error.message);
    return Promise.reject(error);
});
// å“åº”æ‹¦æˆªå™¨ - é”™è¯¯å¤„ç†å’Œæ—¥å¿—
aiClient.interceptors.response.use((response) => {
    const requestId = response.config.headers['X-Request-ID'];
    console.log(`âœ… AI API å“åº” [${requestId}]:`, {
        status: response.status,
        model: response.data.model,
        usage: response.data.usage
    });
    return response;
}, (error) => {
    const requestId = error.config?.headers?.['X-Request-ID'] || 'unknown';
    if (error.response) {
        console.error(`âŒ AI API é”™è¯¯å“åº” [${requestId}]:`, {
            status: error.response.status,
            statusText: error.response.statusText,
            data: error.response.data
        });
    }
    else if (error.request) {
        console.error(`âŒ AI API ç½‘ç»œé”™è¯¯ [${requestId}]:`, {
            code: error.code,
            message: error.message
        });
    }
    else {
        console.error(`âŒ AI API é…ç½®é”™è¯¯ [${requestId}]:`, error.message);
    }
    return Promise.reject(error);
});
class AIService {
    // è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
    getSupportedModels() {
        return Object.values(SUPPORTED_MODELS);
    }
    // è·å–ç‰¹å®šæ¨¡å‹é…ç½®
    getModelConfig(modelId) {
        return SUPPORTED_MODELS[modelId] || null;
    }
    // éªŒè¯æ¨¡å‹æ˜¯å¦å¯ç”¨
    async validateModel(modelId) {
        const config = this.getModelConfig(modelId);
        if (!config)
            return false;
        try {
            // åˆ›å»ºé’ˆå¯¹ç‰¹å®šæ¨¡å‹çš„å®¢æˆ·ç«¯
            const client = this.createModelClient(config);
            // å‘é€æµ‹è¯•è¯·æ±‚
            const response = await client.post('/chat/completions', {
                model: modelId,
                messages: [{ role: 'user', content: 'Hi' }],
                max_tokens: 1,
                temperature: 0
            });
            return response.status === 200;
        }
        catch (error) {
            console.error(`æ¨¡å‹ ${modelId} éªŒè¯å¤±è´¥:`, error);
            return false;
        }
    }
    // åˆ›å»ºæ¨¡å‹ä¸“ç”¨å®¢æˆ·ç«¯
    createModelClient(config) {
        return axios_1.default.create({
            baseURL: config.baseUrl || NEWAPI_BASE_URL,
            headers: {
                'Authorization': `Bearer ${config.apiKey || NEWAPI_KEY}`,
                'Content-Type': 'application/json',
                'Connection': 'keep-alive'
            },
            timeout: CONNECTION_POOL_CONFIG.timeout,
            maxRedirects: 3,
            httpAgent: new (require('http').Agent)({
                keepAlive: CONNECTION_POOL_CONFIG.keepAlive,
                keepAliveMsecs: CONNECTION_POOL_CONFIG.keepAliveMsecs,
                maxSockets: CONNECTION_POOL_CONFIG.maxConnections,
                timeout: CONNECTION_POOL_CONFIG.timeout
            }),
            httpsAgent: new (require('https').Agent)({
                keepAlive: CONNECTION_POOL_CONFIG.keepAlive,
                keepAliveMsecs: CONNECTION_POOL_CONFIG.keepAliveMsecs,
                maxSockets: CONNECTION_POOL_CONFIG.maxConnections,
                timeout: CONNECTION_POOL_CONFIG.timeout
            })
        });
    }
    // ç”ŸæˆèŠå¤©å›å¤ï¼ˆå¸¦é‡è¯•æœºåˆ¶å’Œå¤šæ¨¡å‹æ”¯æŒï¼‰
    async generateChatResponse(options) {
        const { sessionId, userId, characterId, messages, model = DEFAULT_MODEL, temperature = DEFAULT_TEMPERATURE, maxTokens = DEFAULT_MAX_TOKENS, stream = true } = options;
        return await withRetry(async () => {
            // è·å–è§’è‰²ä¿¡æ¯æ¥æ„å»ºç³»ç»Ÿæç¤º
            let systemPrompt = 'ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚';
            if (characterId) {
                const character = await server_1.prisma.character.findUnique({
                    where: { id: characterId }
                });
                if (character) {
                    systemPrompt = this.buildCharacterPrompt(character);
                }
            }
            // æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            const apiMessages = [
                { role: 'system', content: systemPrompt },
                ...messages
            ];
            // è°ƒç”¨ NewAPI
            const response = await aiClient.post('/chat/completions', {
                model,
                messages: apiMessages,
                temperature,
                max_tokens: maxTokens,
                stream
            });
            if (stream) {
                // è¿”å›æµå¼å“åº”
                return response.data;
            }
            else {
                // è¿”å›å®Œæ•´å“åº”
                const content = response.data.choices[0]?.message?.content || '';
                // å¦‚æœå†…å®¹ä¸ºç©ºï¼Œåˆ™æŠ›å‡ºé”™è¯¯é‡è¯•
                if (!content.trim()) {
                    throw new Error('AI è¿”å›ç©ºå“åº”');
                }
                return {
                    content,
                    model: response.data.model,
                    usage: response.data.usage
                };
            }
        }, `AI èŠå¤©ç”Ÿæˆ (${model})`);
    }
    // æµå¼ç”ŸæˆèŠå¤©å›å¤
    async *generateChatStream(options) {
        const { sessionId, userId, characterId, messages, model = DEFAULT_MODEL, temperature = DEFAULT_TEMPERATURE, maxTokens = DEFAULT_MAX_TOKENS } = options;
        try {
            // è·å–è§’è‰²ä¿¡æ¯
            let systemPrompt = 'ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚';
            if (characterId) {
                const character = await server_1.prisma.character.findUnique({
                    where: { id: characterId }
                });
                if (character) {
                    systemPrompt = this.buildCharacterPrompt(character);
                }
            }
            // æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            const apiMessages = [
                { role: 'system', content: systemPrompt },
                ...messages
            ];
            // è°ƒç”¨ NewAPI æµå¼æ¥å£
            const response = await aiClient.post('/chat/completions', {
                model,
                messages: apiMessages,
                temperature,
                max_tokens: maxTokens,
                stream: true
            }, {
                responseType: 'stream'
            });
            // å¤„ç†æµå¼å“åº”
            const stream = response.data;
            let buffer = '';
            for await (const chunk of stream) {
                buffer += chunk.toString();
                const lines = buffer.split('\n');
                buffer = lines.pop() || '';
                for (const line of lines) {
                    if (line.startsWith('data: ')) {
                        const data = line.slice(6);
                        if (data === '[DONE]') {
                            return;
                        }
                        try {
                            const parsed = JSON.parse(data);
                            const content = parsed.choices[0]?.delta?.content;
                            if (content) {
                                yield content;
                            }
                        }
                        catch (e) {
                            console.error('è§£ææµæ•°æ®å¤±è´¥:', e);
                        }
                    }
                }
            }
        }
        catch (error) {
            console.error('æµå¼ç”Ÿæˆå¤±è´¥:', error);
            throw new Error('æµå¼ç”Ÿæˆå¤±è´¥: ' + error.message);
        }
    }
    // æ„å»ºè§’è‰²æç¤ºè¯
    buildCharacterPrompt(character) {
        let prompt = `ä½ æ˜¯${character.name}ã€‚`;
        if (character.description) {
            prompt += `\n\nè§’è‰²æè¿°ï¼š${character.description}`;
        }
        if (character.personality) {
            prompt += `\n\næ€§æ ¼ç‰¹å¾ï¼š${character.personality}`;
        }
        if (character.backstory) {
            prompt += `\n\nèƒŒæ™¯æ•…äº‹ï¼š${character.backstory}`;
        }
        if (character.speakingStyle) {
            prompt += `\n\nè¯´è¯é£æ ¼ï¼š${character.speakingStyle}`;
        }
        if (character.systemPrompt) {
            prompt += `\n\n${character.systemPrompt}`;
        }
        prompt += '\n\nè¯·æ ¹æ®ä»¥ä¸Šè§’è‰²è®¾å®šè¿›è¡Œå¯¹è¯ï¼Œä¿æŒè§’è‰²çš„ä¸€è‡´æ€§ã€‚';
        return prompt;
    }
    // ç”Ÿæˆè§’è‰²è®¾å®šï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    async generateCharacterProfile(name, tags = []) {
        return await withRetry(async () => {
            const prompt = `è¯·ä¸ºä¸€ä¸ªåä¸º"${name}"çš„AIè§’è‰²ç”Ÿæˆè¯¦ç»†çš„è§’è‰²è®¾å®šã€‚
æ ‡ç­¾ï¼š${tags.join('ã€') || 'æ— ç‰¹å®šæ ‡ç­¾'}

è¯·ç”Ÿæˆä»¥ä¸‹å†…å®¹ï¼š
1. è§’è‰²æè¿°ï¼ˆ50-100å­—ï¼‰
2. æ€§æ ¼ç‰¹å¾ï¼ˆ30-50å­—ï¼‰
3. èƒŒæ™¯æ•…äº‹ï¼ˆ100-200å­—ï¼‰
4. è¯´è¯é£æ ¼ï¼ˆ30-50å­—ï¼‰
5. åˆå§‹æ¶ˆæ¯ï¼ˆä¸€å¥å‹å¥½çš„é—®å€™ï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- description
- personality
- backstory
- speakingStyle
- firstMessage`;
            const response = await aiClient.post('/chat/completions', {
                model: DEFAULT_MODEL,
                messages: [
                    {
                        role: 'system',
                        content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§’è‰²è®¾è®¡å¸ˆï¼Œæ“…é•¿åˆ›å»ºæœ‰è¶£ä¸”ç‹¬ç‰¹çš„AIè§’è‰²ã€‚è¯·ç›´æ¥è¿”å›JSONæ ¼å¼çš„å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–è¯´æ˜ã€‚'
                    },
                    {
                        role: 'user',
                        content: prompt
                    }
                ],
                temperature: 0.8,
                max_tokens: 800
            });
            const content = response.data.choices[0]?.message?.content || '{}';
            try {
                // å°è¯•è§£æJSON
                const parsed = JSON.parse(content);
                // éªŒè¯å¿…è¦å­—æ®µ
                if (!parsed.description || !parsed.personality || !parsed.firstMessage) {
                    throw new Error('AI è¿”å›çš„è§’è‰²è®¾å®šç¼ºå°‘å¿…è¦å­—æ®µ');
                }
                return parsed;
            }
            catch (e) {
                // å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
                console.error('è§£æè§’è‰²è®¾å®šJSONå¤±è´¥:', e);
                return {
                    description: `${name}æ˜¯ä¸€ä¸ªç‹¬ç‰¹è€Œæœ‰è¶£çš„AIè§’è‰²ã€‚`,
                    personality: 'å‹å¥½ã€èªæ˜ã€å¯Œæœ‰åŒæƒ…å¿ƒ',
                    backstory: `${name}æ¥è‡ªä¸€ä¸ªå……æ»¡æƒ³è±¡åŠ›çš„ä¸–ç•Œï¼Œæ‹¥æœ‰ä¸°å¯Œçš„ç»å†å’Œæ•…äº‹ã€‚`,
                    speakingStyle: 'æ¸©å’Œå‹å–„ï¼Œå¶å°”å¹½é»˜',
                    firstMessage: `ä½ å¥½ï¼æˆ‘æ˜¯${name}ï¼Œå¾ˆé«˜å…´è®¤è¯†ä½ ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ`
                };
            }
        }, `è§’è‰²è®¾å®šç”Ÿæˆ (${name})`);
    }
    // æ£€æŸ¥APIçŠ¶æ€ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    async checkAPIStatus() {
        try {
            return await withRetry(async () => {
                const response = await aiClient.get('/models');
                return {
                    available: true,
                    models: response.data.data || [],
                    responseTime: Date.now()
                };
            }, 'API çŠ¶æ€æ£€æŸ¥');
        }
        catch (error) {
            return {
                available: false,
                error: error.message || 'APIä¸å¯ç”¨',
                lastChecked: Date.now()
            };
        }
    }
    // å¥åº·æ£€æŸ¥
    async healthCheck() {
        const startTime = Date.now();
        try {
            const status = await this.checkAPIStatus();
            const responseTime = Date.now() - startTime;
            return {
                healthy: status.available,
                responseTime,
                details: status
            };
        }
        catch (error) {
            return {
                healthy: false,
                responseTime: Date.now() - startTime,
                error: error.message
            };
        }
    }
    // è®¡ç®—tokensæ•°é‡ï¼ˆç®€å•ä¼°ç®—ï¼‰
    estimateTokens(text) {
        // ä¸­æ–‡å¤§çº¦1.5ä¸ªå­—ç¬¦ä¸€ä¸ªtokenï¼Œè‹±æ–‡å¤§çº¦4ä¸ªå­—ç¬¦ä¸€ä¸ªtoken
        const chineseChars = (text.match(/[\u4e00-\u9fa5]/g) || []).length;
        const englishChars = text.length - chineseChars;
        return Math.ceil(chineseChars / 1.5 + englishChars / 4);
    }
    // è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹
    async getAvailableModels() {
        return Object.values(SUPPORTED_MODELS);
    }
    // è·å–ç‰¹å®šæ¨¡å‹ä¿¡æ¯
    async getModelInfo(modelId) {
        return SUPPORTED_MODELS[modelId] || null;
    }
    // è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯
    async getModelStats(modelId, startDate) {
        try {
            // ä»æ¶ˆæ¯è¡¨è·å–ç»Ÿè®¡ä¿¡æ¯
            const messages = await server_1.prisma.message.findMany({
                where: {
                    createdAt: {
                        gte: startDate
                    },
                    // å‡è®¾æˆ‘ä»¬åœ¨metadataä¸­å­˜å‚¨äº†æ¨¡å‹ä¿¡æ¯
                    metadata: {
                        contains: modelId
                    }
                },
                select: {
                    tokens: true,
                    createdAt: true,
                    metadata: true
                }
            });
            const totalRequests = messages.length;
            const totalTokensUsed = messages.reduce((sum, msg) => sum + (msg.tokens || 0), 0);
            // è®¡ç®—æˆåŠŸç‡ï¼ˆç®€åŒ–å¤„ç†ï¼Œå‡è®¾æ‰€æœ‰è®°å½•éƒ½æ˜¯æˆåŠŸçš„ï¼‰
            const successfulRequests = totalRequests;
            const failedRequests = 0;
            // è®¡ç®—å¹³å‡å“åº”æ—¶é—´ï¼ˆè¿™é‡Œéœ€è¦ä»æ—¥å¿—æˆ–å…¶ä»–åœ°æ–¹è·å–ï¼‰
            const averageResponseTime = 1500; // é»˜è®¤å€¼ï¼Œå®é™…åº”è¯¥ä»æ€§èƒ½ç›‘æ§ä¸­è·å–
            return {
                totalRequests,
                successfulRequests,
                failedRequests,
                totalTokensUsed,
                averageResponseTime
            };
        }
        catch (error) {
            console.error('è·å–æ¨¡å‹ç»Ÿè®¡å¤±è´¥:', error);
            return {
                totalRequests: 0,
                successfulRequests: 0,
                failedRequests: 0,
                totalTokensUsed: 0,
                averageResponseTime: 0
            };
        }
    }
}
exports.aiService = new AIService();
//# sourceMappingURL=ai-backup.js.map