import axios, { AxiosInstance, AxiosError } from 'axios'
import { prisma } from '../server'

// å¤šæ¨¡å‹ LLM é…ç½® - ä»ç¯å¢ƒå˜é‡è·å–
const NEWAPI_BASE_URL = process.env.NEWAPI_BASE_URL || 'https://ttkk.inping.com/v1'
const NEWAPI_KEY = process.env.NEWAPI_KEY
const DEFAULT_MODEL = process.env.DEFAULT_MODEL || 'grok-3'
const DEFAULT_MAX_TOKENS = parseInt(process.env.NEWAPI_MAX_TOKENS || '4000')
const DEFAULT_TEMPERATURE = parseFloat(process.env.NEWAPI_TEMPERATURE || '0.7')

// æ”¯æŒçš„æ¨¡å‹é…ç½®
interface ModelConfig {
  id: string
  name: string
  provider: 'newapi' | 'openai' | 'anthropic' | 'google'
  baseUrl?: string
  apiKey?: string
  maxTokens: number
  temperature: number
  description: string
  features: string[]
  pricePer1k?: number
}

const SUPPORTED_MODELS: Record<string, ModelConfig> = {
  'grok-3': {
    id: 'grok-3',
    name: 'Grok-3',
    provider: 'newapi',
    baseUrl: NEWAPI_BASE_URL,
    apiKey: NEWAPI_KEY,
    maxTokens: 4000,
    temperature: 0.7,
    description: 'å¼ºå¤§çš„å¯¹è¯æ¨¡å‹ï¼Œæ“…é•¿åˆ›æ„å†™ä½œå’Œå¤æ‚æ¨ç†',
    features: ['å¯¹è¯', 'åˆ›æ„å†™ä½œ', 'ä»£ç ç”Ÿæˆ', 'é€»è¾‘æ¨ç†'],
    pricePer1k: 0.01
  },
  'gpt-4': {
    id: 'gpt-4',
    name: 'GPT-4',
    provider: 'newapi',
    baseUrl: NEWAPI_BASE_URL,
    apiKey: NEWAPI_KEY,
    maxTokens: 8000,
    temperature: 0.7,
    description: 'OpenAI æœ€å¼ºå¤§çš„æ¨¡å‹ï¼Œé€‚åˆå¤æ‚ä»»åŠ¡',
    features: ['å¯¹è¯', 'åˆ†æ', 'åˆ›ä½œ', 'ç¼–ç¨‹', 'æ¨ç†'],
    pricePer1k: 0.03
  },
  'gpt-3.5-turbo': {
    id: 'gpt-3.5-turbo',
    name: 'GPT-3.5 Turbo',
    provider: 'newapi',
    baseUrl: NEWAPI_BASE_URL,
    apiKey: NEWAPI_KEY,
    maxTokens: 4000,
    temperature: 0.7,
    description: 'å¿«é€Ÿå“åº”ï¼Œæˆæœ¬è¾ƒä½çš„å¯¹è¯æ¨¡å‹',
    features: ['å¯¹è¯', 'æ–‡æœ¬ç”Ÿæˆ', 'æ€»ç»“'],
    pricePer1k: 0.002
  },
  'claude-3': {
    id: 'claude-3',
    name: 'Claude 3',
    provider: 'newapi',
    baseUrl: NEWAPI_BASE_URL,
    apiKey: NEWAPI_KEY,
    maxTokens: 4000,
    temperature: 0.7,
    description: 'Anthropic çš„å®‰å…¨å¯é å¯¹è¯æ¨¡å‹',
    features: ['å¯¹è¯', 'åˆ†æ', 'å®‰å…¨å›å¤'],
    pricePer1k: 0.015
  },
  'gemini-pro': {
    id: 'gemini-pro',
    name: 'Gemini Pro',
    provider: 'newapi',
    baseUrl: NEWAPI_BASE_URL,
    apiKey: NEWAPI_KEY,
    maxTokens: 2048,
    temperature: 0.7,
    description: 'Google çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ”¯æŒæ–‡æœ¬å’Œå›¾åƒ',
    features: ['å¯¹è¯', 'å¤šæ¨¡æ€', 'åˆ†æ'],
    pricePer1k: 0.001
  }
}

// è¿æ¥ç¨³å®šæ€§é…ç½®
const RETRY_CONFIG = {
  maxRetries: 3,
  baseDelay: 1000,
  maxDelay: 10000,
  backoffMultiplier: 2
}

const CONNECTION_POOL_CONFIG = {
  maxConnections: 10,
  timeout: 30000,
  keepAlive: true,
  keepAliveMsecs: 60000
}

// éªŒè¯å…³é”®é…ç½®
if (!NEWAPI_KEY) {
  console.error('âŒ é”™è¯¯: NEWAPI_KEY æœªé…ç½®ï¼ŒAIæœåŠ¡å°†ä¸å¯ç”¨')
} else {
  console.log('âœ… Grok-3 LLM é…ç½®å·²åŠ è½½')
  console.log(`   Base URL: ${NEWAPI_BASE_URL}`)
  console.log(`   Model: ${DEFAULT_MODEL}`)
  console.log(`   Max Tokens: ${DEFAULT_MAX_TOKENS}`)
  console.log(`   Temperature: ${DEFAULT_TEMPERATURE}`)
  console.log(`   Retry Config: ${RETRY_CONFIG.maxRetries} retries, ${RETRY_CONFIG.baseDelay}ms base delay`)
}

// å·¥å…·å‡½æ•°
const calculateDelay = (attempt: number): number => {
  const delay = RETRY_CONFIG.baseDelay * Math.pow(RETRY_CONFIG.backoffMultiplier, attempt)
  return Math.min(delay, RETRY_CONFIG.maxDelay)
}

const sleep = (ms: number): Promise<void> => new Promise(resolve => setTimeout(resolve, ms))

async function withRetry<T>(
  operation: () => Promise<T>,
  context: string = 'AI Service'
): Promise<T> {
  let lastError: Error | null = null

  for (let attempt = 0; attempt <= RETRY_CONFIG.maxRetries; attempt++) {
    try {
      const result = await operation()
      if (attempt > 0) {
        console.log(`âœ… ${context}: é‡è¯•æˆåŠŸ (ç¬¬ ${attempt + 1} æ¬¡å°è¯•)`)
      }
      return result
    } catch (error: any) {
      lastError = error

      const isRetriable = error.code === 'ECONNRESET' ||
                         error.code === 'ECONNREFUSED' ||
                         error.code === 'ETIMEDOUT' ||
                         error.code === 'ENOTFOUND' ||
                         (error.response?.status >= 500 && error.response?.status < 600) ||
                         error.response?.status === 429

      if (!isRetriable || attempt === RETRY_CONFIG.maxRetries) {
        console.error(`âŒ ${context}: æ“ä½œå¤±è´¥ (ç¬¬ ${attempt + 1} æ¬¡å°è¯•)`, {
          error: error.message,
          code: error.code,
          status: error.response?.status
        })
        break
      }

      const delay = calculateDelay(attempt)
      console.warn(`âš ï¸ ${context}: ç¬¬ ${attempt + 1} æ¬¡å°è¯•å¤±è´¥ï¼Œ${delay}ms åé‡è¯•`, {
        error: error.message,
        nextRetryIn: `${delay}ms`
      })

      await sleep(delay)
    }
  }

  throw lastError || new Error(`${context}: æ“ä½œå¤±è´¥`)
}

// åˆ›å»ºAIå®¢æˆ·ç«¯
const aiClient: AxiosInstance = axios.create({
  baseURL: NEWAPI_BASE_URL,
  headers: {
    'Authorization': `Bearer ${NEWAPI_KEY}`,
    'Content-Type': 'application/json',
    'Connection': 'keep-alive'
  },
  timeout: CONNECTION_POOL_CONFIG.timeout,
  maxRedirects: 3,
  httpAgent: new (require('http').Agent)({
    keepAlive: CONNECTION_POOL_CONFIG.keepAlive,
    keepAliveMsecs: CONNECTION_POOL_CONFIG.keepAliveMsecs,
    maxSockets: CONNECTION_POOL_CONFIG.maxConnections,
    timeout: CONNECTION_POOL_CONFIG.timeout
  }),
  httpsAgent: new (require('https').Agent)({
    keepAlive: CONNECTION_POOL_CONFIG.keepAlive,
    keepAliveMsecs: CONNECTION_POOL_CONFIG.keepAliveMsecs,
    maxSockets: CONNECTION_POOL_CONFIG.maxConnections,
    timeout: CONNECTION_POOL_CONFIG.timeout
  })
})

// æ‹¦æˆªå™¨
aiClient.interceptors.request.use(
  (config) => {
    const requestId = Math.random().toString(36).substr(2, 9)
    config.headers['X-Request-ID'] = requestId
    console.log(`ğŸš€ AI API è¯·æ±‚ [${requestId}]:`, {
      method: config.method?.toUpperCase(),
      url: config.url,
      timeout: config.timeout
    })
    return config
  },
  (error) => {
    console.error('âŒ AI API è¯·æ±‚é…ç½®é”™è¯¯:', error.message)
    return Promise.reject(error)
  }
)

aiClient.interceptors.response.use(
  (response) => {
    const requestId = response.config.headers['X-Request-ID']
    console.log(`âœ… AI API å“åº” [${requestId}]:`, {
      status: response.status,
      model: response.data.model,
      usage: response.data.usage
    })
    return response
  },
  (error: AxiosError) => {
    const requestId = error.config?.headers?.['X-Request-ID'] || 'unknown'

    if (error.response) {
      console.error(`âŒ AI API é”™è¯¯å“åº” [${requestId}]:`, {
        status: error.response.status,
        statusText: error.response.statusText,
        data: error.response.data
      })
    } else if (error.request) {
      console.error(`âŒ AI API ç½‘ç»œé”™è¯¯ [${requestId}]:`, {
        code: error.code,
        message: error.message
      })
    } else {
      console.error(`âŒ AI API é…ç½®é”™è¯¯ [${requestId}]:`, error.message)
    }

    return Promise.reject(error)
  }
)

// æ¥å£å®šä¹‰
export interface GenerateOptions {
  sessionId: string
  userId: string
  characterId?: string
  messages: Array<{
    role: 'system' | 'user' | 'assistant'
    content: string
  }>
  model?: string
  temperature?: number
  maxTokens?: number
  stream?: boolean
}

export interface StreamChunk {
  id: string
  object: string
  created: number
  model: string
  choices: Array<{
    index: number
    delta: {
      role?: string
      content?: string
    }
    finish_reason: string | null
  }>
}

// AIæœåŠ¡ç±»
class AIService {
  // è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹
  async getAvailableModels(): Promise<ModelConfig[]> {
    return Object.values(SUPPORTED_MODELS)
  }

  // è·å–ç‰¹å®šæ¨¡å‹ä¿¡æ¯
  async getModelInfo(modelId: string): Promise<ModelConfig | null> {
    return SUPPORTED_MODELS[modelId] || null
  }

  // è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯
  async getModelStats(modelId: string, startDate: Date) {
    try {
      const messages = await prisma.message.findMany({
        where: {
          createdAt: {
            gte: startDate
          },
          metadata: {
            contains: modelId
          }
        },
        select: {
          tokens: true,
          createdAt: true,
          metadata: true
        }
      })

      const totalRequests = messages.length
      const totalTokensUsed = messages.reduce((sum, msg) => sum + (msg.tokens || 0), 0)
      const successfulRequests = totalRequests
      const failedRequests = 0
      const averageResponseTime = 1500

      return {
        totalRequests,
        successfulRequests,
        failedRequests,
        totalTokensUsed,
        averageResponseTime
      }
    } catch (error) {
      console.error('è·å–æ¨¡å‹ç»Ÿè®¡å¤±è´¥:', error)
      return {
        totalRequests: 0,
        successfulRequests: 0,
        failedRequests: 0,
        totalTokensUsed: 0,
        averageResponseTime: 0
      }
    }
  }

  // åˆ›å»ºæ¨¡å‹ä¸“ç”¨å®¢æˆ·ç«¯
  private createModelClient(config: ModelConfig): AxiosInstance {
    return axios.create({
      baseURL: config.baseUrl || NEWAPI_BASE_URL,
      headers: {
        'Authorization': `Bearer ${config.apiKey || NEWAPI_KEY}`,
        'Content-Type': 'application/json',
        'Connection': 'keep-alive'
      },
      timeout: CONNECTION_POOL_CONFIG.timeout
    })
  }

  // éªŒè¯æ¨¡å‹æ˜¯å¦å¯ç”¨
  async validateModel(modelId: string): Promise<boolean> {
    const config = this.getModelConfig(modelId)
    if (!config) return false

    try {
      const client = this.createModelClient(config)
      const response = await client.post('/chat/completions', {
        model: modelId,
        messages: [{ role: 'user', content: 'Hi' }],
        max_tokens: 1,
        temperature: 0
      })
      return response.status === 200
    } catch (error) {
      console.error(`æ¨¡å‹ ${modelId} éªŒè¯å¤±è´¥:`, error)
      return false
    }
  }

  // è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
  getSupportedModels(): ModelConfig[] {
    return Object.values(SUPPORTED_MODELS)
  }

  // è·å–ç‰¹å®šæ¨¡å‹é…ç½®
  getModelConfig(modelId: string): ModelConfig | null {
    return SUPPORTED_MODELS[modelId] || null
  }

  // ç”ŸæˆèŠå¤©å›å¤
  async generateChatResponse(options: any) {
    const {
      model = DEFAULT_MODEL,
      messages,
      maxTokens = DEFAULT_MAX_TOKENS,
      temperature = DEFAULT_TEMPERATURE
    } = options

    return await withRetry(async () => {
      const response = await aiClient.post('/chat/completions', {
        model,
        messages,
        max_tokens: maxTokens,
        temperature
      })

      const content = response.data.choices[0]?.message?.content || ''
      const tokensUsed = response.data.usage?.total_tokens || 0

      return {
        content,
        tokensUsed,
        model: response.data.model
      }
    }, `èŠå¤©ç”Ÿæˆ (${model})`)
  }



  // ç”Ÿæˆè§’è‰²è®¾å®š
  async generateCharacterProfile(prompt: string, options: any = {}) {
    const { name, tags = [], style, personality, background } = options

    return await withRetry(async () => {
      const generatePrompt = `è¯·ä¸ºä¸€ä¸ªåä¸º"${name}"çš„AIè§’è‰²ç”Ÿæˆè¯¦ç»†çš„è§’è‰²è®¾å®šã€‚
æè¿°ï¼š${prompt}
é£æ ¼ï¼š${style || 'åŠ¨æ¼«'}
æ€§æ ¼ï¼š${personality || 'å¼€æœ—'}
èƒŒæ™¯ï¼š${background || 'ç°ä»£'}
æ ‡ç­¾ï¼š${tags.join('ã€') || 'æ— ç‰¹å®šæ ‡ç­¾'}

è¯·ç”Ÿæˆä»¥ä¸‹å†…å®¹ï¼š
1. è§’è‰²æè¿°ï¼ˆ50-100å­—ï¼‰
2. æ€§æ ¼ç‰¹å¾ï¼ˆ30-50å­—ï¼‰
3. èƒŒæ™¯æ•…äº‹ï¼ˆ100-200å­—ï¼‰
4. è¯´è¯é£æ ¼ï¼ˆ30-50å­—ï¼‰
5. åˆå§‹æ¶ˆæ¯ï¼ˆä¸€å¥å‹å¥½çš„é—®å€™ï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- description
- personality
- backstory
- speakingStyle
- firstMessage`

      const response = await aiClient.post('/chat/completions', {
        model: DEFAULT_MODEL,
        messages: [
          {
            role: 'system',
            content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§’è‰²è®¾è®¡å¸ˆï¼Œæ“…é•¿åˆ›å»ºæœ‰è¶£ä¸”ç‹¬ç‰¹çš„AIè§’è‰²ã€‚è¯·ç›´æ¥è¿”å›JSONæ ¼å¼çš„å†…å®¹ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–è¯´æ˜ã€‚'
          },
          {
            role: 'user',
            content: generatePrompt
          }
        ],
        temperature: 0.8,
        max_tokens: 800
      })

      const content = response.data.choices[0]?.message?.content || '{}'

      try {
        const parsed = JSON.parse(content)
        if (!parsed.description || !parsed.personality || !parsed.firstMessage) {
          throw new Error('AI è¿”å›çš„è§’è‰²è®¾å®šç¼ºå°‘å¿…è¦å­—æ®µ')
        }
        return parsed
      } catch (e) {
        console.error('è§£æè§’è‰²è®¾å®šJSONå¤±è´¥:', e)
        return {
          description: `${name}æ˜¯ä¸€ä¸ªç‹¬ç‰¹è€Œæœ‰è¶£çš„AIè§’è‰²ã€‚`,
          personality: 'å‹å¥½ã€èªæ˜ã€å¯Œæœ‰åŒæƒ…å¿ƒ',
          backstory: `${name}æ¥è‡ªä¸€ä¸ªå……æ»¡æƒ³è±¡åŠ›çš„ä¸–ç•Œï¼Œæ‹¥æœ‰ä¸°å¯Œçš„ç»å†å’Œæ•…äº‹ã€‚`,
          speakingStyle: 'æ¸©å’Œå‹å–„ï¼Œå¶å°”å¹½é»˜',
          firstMessage: `ä½ å¥½ï¼æˆ‘æ˜¯${name}ï¼Œå¾ˆé«˜å…´è®¤è¯†ä½ ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ`
        }
      }
    }, `è§’è‰²è®¾å®šç”Ÿæˆ (${name})`)
  }

  // å¥åº·æ£€æŸ¥
  async healthCheck() {
    const startTime = Date.now()

    try {
      const status = await this.checkAPIStatus()
      const responseTime = Date.now() - startTime

      return {
        healthy: status.available,
        responseTime,
        details: status
      }
    } catch (error: any) {
      return {
        healthy: false,
        responseTime: Date.now() - startTime,
        error: error.message
      }
    }
  }

  // æ£€æŸ¥APIçŠ¶æ€
  async checkAPIStatus() {
    try {
      return await withRetry(async () => {
        const response = await aiClient.get('/models')
        return {
          available: true,
          models: response.data.data || [],
          responseTime: Date.now()
        }
      }, 'API çŠ¶æ€æ£€æŸ¥')
    } catch (error: any) {
      return {
        available: false,
        error: error.message || 'APIä¸å¯ç”¨',
        lastChecked: Date.now()
      }
    }
  }

  // è®¡ç®—tokensæ•°é‡
  estimateTokens(text: string): number {
    const chineseChars = (text.match(/[\u4e00-\u9fa5]/g) || []).length
    const englishChars = text.length - chineseChars
    return Math.ceil(chineseChars / 1.5 + englishChars / 4)
  }

  // æ„å»ºè§’è‰²æç¤ºè¯
  private buildCharacterPrompt(character: any): string {
    let prompt = `ä½ æ˜¯${character.name}ã€‚`

    if (character.description) {
      prompt += `\n\nè§’è‰²æè¿°ï¼š${character.description}`
    }

    if (character.personality) {
      prompt += `\n\næ€§æ ¼ç‰¹å¾ï¼š${character.personality}`
    }

    if (character.backstory) {
      prompt += `\n\nèƒŒæ™¯æ•…äº‹ï¼š${character.backstory}`
    }

    if (character.speakingStyle) {
      prompt += `\n\nè¯´è¯é£æ ¼ï¼š${character.speakingStyle}`
    }

    if (character.systemPrompt) {
      prompt += `\n\n${character.systemPrompt}`
    }

    prompt += '\n\nè¯·æ ¹æ®ä»¥ä¸Šè§’è‰²è®¾å®šè¿›è¡Œå¯¹è¯ï¼Œä¿æŒè§’è‰²çš„ä¸€è‡´æ€§ã€‚'
    return prompt
  }

  // æµå¼ç”ŸæˆèŠå¤©å›å¤
  async *generateChatStream(options: GenerateOptions) {
    const {
      sessionId,
      userId,
      characterId,
      messages,
      model = DEFAULT_MODEL,
      temperature = DEFAULT_TEMPERATURE,
      maxTokens = DEFAULT_MAX_TOKENS
    } = options

    try {
      // è·å–è§’è‰²ä¿¡æ¯
      let systemPrompt = 'ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚'

      if (characterId) {
        const character = await prisma.character.findUnique({
          where: { id: characterId }
        })

        if (character) {
          systemPrompt = this.buildCharacterPrompt(character)
        }
      }

      // æ„å»ºæ¶ˆæ¯åˆ—è¡¨
      const apiMessages = [
        { role: 'system' as const, content: systemPrompt },
        ...messages
      ]

      // è°ƒç”¨ NewAPI æµå¼æ¥å£
      const response = await aiClient.post('/chat/completions', {
        model,
        messages: apiMessages,
        temperature,
        max_tokens: maxTokens,
        stream: true
      }, {
        responseType: 'stream'
      })

      // å¤„ç†æµå¼å“åº”
      const stream = response.data
      let buffer = ''

      for await (const chunk of stream) {
        buffer += chunk.toString()
        const lines = buffer.split('\n')
        buffer = lines.pop() || ''

        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6)

            if (data === '[DONE]') {
              return
            }

            try {
              const parsed = JSON.parse(data) as StreamChunk
              const content = parsed.choices[0]?.delta?.content

              if (content) {
                yield content
              }
            } catch (e) {
              console.error('è§£ææµæ•°æ®å¤±è´¥:', e)
            }
          }
        }
      }
    } catch (error: any) {
      console.error('æµå¼ç”Ÿæˆå¤±è´¥:', error)
      throw new Error('æµå¼ç”Ÿæˆå¤±è´¥: ' + error.message)
    }
  }
}

export const aiService = new AIService()
export { ModelConfig }
